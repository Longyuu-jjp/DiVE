<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer.">
  <meta name="keywords" content="DiVE, Multi-View, Generation, DiT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Longyuu-jjp">Junpeng Jiang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://dblp.org/pid/360/9951.html">Gangyi Hong</a><sup>3,2</sup>,</span>
            <span class="author-block">
              <a href="https://miaozhang0525.github.io/">Miao Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=tF5tWsMAAAAJ&hl=zh-CN">Hengtong Hu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/ZhanKunLiAuto">Kun Zhan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://rshaojimmy.github.io/OrionLab/">Rui Shao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://liqiangnie.github.io/index.html">Liqiang Nie</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harbin Institute of Technology (Shenzhen),</span>
            <span class="author-block"><sup>2</sup>Li Auto Inc.,</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DiVE</span> won <b>first place</b> in the <a href="https://coda-dataset.github.io/w-coda2024/track2/">
          Corner Case Scene Generation</a> track of ECCV 2024 W-CODA (Multimodal Perception and Comprehension of 
          Corner Cases in Autonomous Driving challenge).
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser_video.SVG"
           alt="teaser-image">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DiVE</span> in qualitative visualizations and comparison of results under different acceleration techniques.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Collecting multi-view driving scenario videos to enhance the performance of 3D 
            visual perception presents significant challenges and incurs substantial costs, 
            making generative models for realistic data an appealing alternative. Yet, videos 
            generated by recent works suffer from poor quality and spatiotemporal consistency, 
            hindering their application in driving perception. To address this gap, we propose 
            <span class="dnerf">DiVE</span>, a diffusion transformer-based generative framework 
            engineered to produce high-fidelity, temporally and cross-view consistent multi-view 
            videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. Specifically, 
            DiVE leverages unified cross-attention and a SketchFormer to exert precise control 
            over multi-modal data, while incorporating a view-inflated attention mechanism that 
            adds no extra parameters, thereby guaranteeing consistency across views. Despite 
            these advancements, synthesizing high-resolution videos under multimodal constraints 
            introduces dual challenges: investigating optimal classifier-free guidance (CFG) 
            coniguration under intricate multi-condition inputs and mitigating excessive 
            computational latency in high-resolution rendering---both remain underexplored in 
            prior research. To resolve these limitations, we introduce two technical innovations: 
            (1) Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition 
            CFG selection while circumventing high computational overhead, and (2) Resolution 
            Progressive Sampling, a training-free acceleration strategy that staggers resolution 
            scaling to reduce high latency due to high resolution. These innovations collectively 
            achieve a 2.62$\times$ speedup with minimal quality degradation. Evaluated on the 
            nuScenes dataset, DiVE achieves state-of-the-art performance in multi-view video 
            generation, yielding photorealistic outputs with exceptional temporal and cross-view 
            coherence. By bridging the gap between synthetic data quality and real-world perceptual 
            requirements, <span class="dnerf">DiVE</span> establishes a robust generative paradigm 
            to catalyze significant advancements in 3D perception systems.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="columns is-full-width">
        <h2 class="title is-3">Proposed Method</h2>
        <img src="./static/images/method.svg"
             alt="Overview of DiVE."/>
      </div>
    </div>

    <!-- Acceleration. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Two Acceleration Tenchniques</h2>

        <!-- MAD. -->
        <h3 class="title is-4">Multi-Control Auxiliary Branch Distillation</h3>
        <div class="content has-text-justified">
          <p>
            It mitigates multi-condition CFG complexity via condition-specific auxiliary 
            branches, enhanced by cross-condition knowledge distillation via mixed-control 
            guidance training.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/mad.svg"
                 alt="Overview of MAD."/>
          </div>
        </div>
        <br/>
        <!--/ MAD. -->

        <!-- RPS. -->
        <h3 class="title is-4">Resolution Progressively Sampling</h3>
        <div class="content has-text-justified">
          <p>
            It's a training-free acceleration strategy, where the inference mode with 
            progressively increasing resolution alleviates the computational burden 
            required for the early-stage inference.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/rps.svg"
                 alt="Overview of RPS."/>
          </div>
        </div>
        <!--/ RPS. -->

      </div>
    </div>
    <!--/ Acceleration. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
