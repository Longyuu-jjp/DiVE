<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer.">
  <meta name="keywords" content="DiVE, Multi-View, Generation, DiT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Longyuu-jjp">Junpeng Jiang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://dblp.org/pid/360/9951.html">Gangyi Hong</a><sup>3,2</sup>,</span>
            <span class="author-block">
              <a href="https://miaozhang0525.github.io/">Miao Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=tF5tWsMAAAAJ&hl=zh-CN">Hengtong Hu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/ZhanKunLiAuto">Kun Zhan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://rshaojimmy.github.io/OrionLab/">Rui Shao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://liqiangnie.github.io/index.html">Liqiang Nie</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harbin Institute of Technology (Shenzhen),</span>
            <span class="author-block"><sup>2</sup>Li Auto Inc.,</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DiVE</span> won <b>first place</b> in the <a href="https://coda-dataset.github.io/w-coda2024/track2/">
          Corner Case Scene Generation</a> track of ECCV 2024 W-CODA (Multimodal Perception and Comprehension of 
          Corner Cases in Autonomous Driving challenge).
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser_video.SVG"
           alt="teaser-image">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DiVE</span> in qualitative visualizations and comparison of results under different acceleration techniques.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Collecting multi-view driving scenario videos to enhance the performance of 3D 
            visual perception presents significant challenges and incurs substantial costs, 
            making generative models for realistic data an appealing alternative. Yet, videos 
            generated by recent works suffer from poor quality and spatiotemporal consistency, 
            hindering their application in driving perception. To address this gap, we propose 
            <span class="dnerf">DiVE</span>, a diffusion transformer-based generative framework 
            engineered to produce high-fidelity, temporally and cross-view consistent multi-view 
            videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. Specifically, 
            DiVE leverages unified cross-attention and a SketchFormer to exert precise control 
            over multi-modal data, while incorporating a view-inflated attention mechanism that 
            adds no extra parameters, thereby guaranteeing consistency across views. Despite 
            these advancements, synthesizing high-resolution videos under multimodal constraints 
            introduces dual challenges: investigating optimal classifier-free guidance (CFG) 
            coniguration under intricate multi-condition inputs and mitigating excessive 
            computational latency in high-resolution rendering---both remain underexplored in 
            prior research. To resolve these limitations, we introduce two technical innovations: 
            (1) Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition 
            CFG selection while circumventing high computational overhead, and (2) Resolution 
            Progressive Sampling, a training-free acceleration strategy that staggers resolution 
            scaling to reduce high latency due to high resolution. These innovations collectively 
            achieve a 2.62$\times$ speedup with minimal quality degradation. Evaluated on the 
            nuScenes dataset, DiVE achieves state-of-the-art performance in multi-view video 
            generation, yielding photorealistic outputs with exceptional temporal and cross-view 
            coherence. By bridging the gap between synthetic data quality and real-world perceptual 
            requirements, <span class="dnerf">DiVE</span> establishes a robust generative paradigm 
            to catalyze significant advancements in 3D perception systems.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Proposed Method</h2>
    <div class="content">
      <img src="./static/images/method.svg"
          alt="Overview of DiVE."/>
    </div>

    <!-- Acceleration. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Two Acceleration Tenchniques</h2>

        <div class="columns">
          <!-- MAD. -->
          <div class="column">
            <div class="content">
              <h3 class="title is-4">Multi-Control Auxiliary Branch Distillation</h3>
              <p>
                It mitigates multi-condition CFG complexity via condition-specific auxiliary 
                branches, enhanced by cross-condition knowledge distillation via mixed-control 
                guidance training.
              </p>
              <img src="./static/images/mad.svg"
                  alt="Overview of MAD."/>
            </div>
          </div>
          <br/>
          <!--/ MAD. -->

          <!-- RPS. -->
          <div class="column">
            <div class="content">
              <h3 class="title is-4">Resolution Progressively Sampling</h3>
              <p>
                It's a training-free acceleration strategy, where the inference mode with 
                progressively increasing resolution alleviates the computational burden 
                required for the early-stage inference.
              </p>
              <img src="./static/images/rps.svg"
                  alt="Overview of RPS."/>
            </div>
          </div>
          <!--/ RPS. -->
        </div>

      </div>
    </div>
    <!--/ Acceleration. -->

  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" style="text-align:center;">
          <img src="./static/images/vis_cmp_magicdrive_panacea.jpg" alt="Image 1" style="width:80%;"/>
        </div>
        <div class="item" style="text-align:center;">
          <img src="./static/images/vis_cmp_magicdrive_panacea_sup.jpg" alt="Image 2" style="width:50%;"/>
        </div>
        <div class="item" style="text-align:center;">
          <img src="./static/images/vis_cmp_magicdrive_panacea_sup_2.jpg" alt="Image 3" style="width:50%;"/>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
